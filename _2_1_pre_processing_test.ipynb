{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"uit-nlp/vietnamese_students_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data#['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['sentence'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['train']#['train'][12]\n",
    "#train_data['sentence']=pd.DataFrame(train_data['sentence']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train']['sentence'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "    \n",
    "# Apply the lower_case function to the 'sentence' column\n",
    "train_data = train_data.map(lambda example: {'sentence': lower_case(example['sentence'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform emoticons to word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sentence: giao viên ko cóa dạy colonsmile\n"
     ]
    }
   ],
   "source": [
    "from _2_0_dictionary import emotion2wordform_dict, wordform2vnese_dict, mispelling_dict, translate_dict, number_dict\n",
    "\n",
    "def emoticon2word(sentence):\n",
    "    words = sentence.split()\n",
    "    converted_words = [emotion2wordform_dict.get(word, word) for word in words]\n",
    "    converted_sentence = ' '.join(converted_words)\n",
    "    return converted_sentence\n",
    "\n",
    "# Test the function\n",
    "sentence = \"giao viên ko cóa dạy :)\"\n",
    "print(\"Converted sentence:\", emoticon2word(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thầy  cô dạy tốt  Trường ok \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(input_string):\n",
    "    # Create a translation table to remove all punctuation characters\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Use the translate method to remove punctuation\n",
    "    cleaned_string = input_string.translate(translator)\n",
    "    \n",
    "    return cleaned_string\n",
    "\n",
    "print(remove_punctuation('thầy , cô dạy tốt . Trường ok.'))\n",
    "#train_data = train_data.map(lambda example: {'sentence': remove_punctuation(example['sentence'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Translate word form of emoticons into VNese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['giao_viên', 'ko', 'cóa', 'dạy', 'colonsmile']\n",
      "Converted sentence: giao_viên ko cóa dạy cười nhẹ\n"
     ]
    }
   ],
   "source": [
    "def word_form2Vnese(sentence):\n",
    "    words = sentence.split()\n",
    "    print(words)\n",
    "    converted_words = [wordform2vnese_dict.get(word, word) for word in words]\n",
    "    converted_sentence = ' '.join(converted_words)\n",
    "    return converted_sentence\n",
    "\n",
    "# Test the function\n",
    "sentence = \"giao_viên ko cóa dạy colonsmile\"\n",
    "print(\"Converted sentence:\", word_form2Vnese(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transform number to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sentence: chấm thầy mười điểm\n"
     ]
    }
   ],
   "source": [
    "# Test cùng với \"chấm thầy 10 điểm\"\n",
    "def number2text(sentence):\n",
    "    words = sentence.split()\n",
    "    converted_words = [number_dict.get(word, word) for word in words]\n",
    "    converted_sentence = ' '.join(converted_words)\n",
    "    return converted_sentence\n",
    "\n",
    "# Test the function\n",
    "sentence = \"chấm thầy 10 điểm\" \n",
    "print(\"Converted sentence:\", number2text(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Spelling correction, acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)\n",
    "\n",
    "sentences = \"giáo_viên\" # nhận thấy token khác nhau, \n",
    "#test cùng với độ tự tin khi dự đoán, nếu input sai chính tả \n",
    "inputs = tokenizer(sentences, padding= True, truncation=True),\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dictionary tiếp cận 1 \n",
    "# temp_dict = {\n",
    "#     'không': ['ko', 'k', 'khum', 'khom', 'khom'],\n",
    "#     'có': ['cóa', 'cóo', 'coá'],\n",
    "#     'giảng viên': ['gv'], \n",
    "#     'giáo_viên' : ['gíao_viên', 'giao_viên']\n",
    "# }\n",
    "\n",
    "# def misspelling2true(sentence, dictionary):\n",
    "#     words = sentence.split()\n",
    "#     converted_words = [convert_list_in_dict(word, dictionary) for word in words]\n",
    "#     converted_sentence = ' '.join(converted_words)\n",
    "#     return converted_sentence\n",
    "\n",
    "# def convert_list_in_dict(word, dictionary):\n",
    "#     for key, values in dictionary.items():\n",
    "#         if word in values:\n",
    "#             return key\n",
    "#     return word\n",
    "\n",
    "# # Test the function with a sentence\n",
    "# sentence_to_convert_back =  'giao viên này dạy ko hay như gv khác'\n",
    "# converted_sentence_back = misspelling2true(sentence_to_convert_back, temp_dict)\n",
    "# print(\"Converted sentence back:\", converted_sentence_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sentence back:  giảng viên dạy  học sinh  trong phòng máy không dễ hiểu xíu nào \n"
     ]
    }
   ],
   "source": [
    "#tiếp cận 2 \n",
    "# dịch từ sai chính tả thành đúng chính tả\n",
    "def mispell2word(sentence, dictionary = mispelling_dict):\n",
    "    sentence = \" \" + sentence.strip() + \" \"\n",
    "    for key, value_list in dictionary.items():\n",
    "        for value in value_list:\n",
    "            sentence = sentence.replace(value, key)\n",
    "    return sentence\n",
    "\n",
    "sentence = 'gv dạy hoc sinh trong phỏng máy k de hieu xíu nào'\n",
    "print(\"Converted sentence back:\", mispell2word(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted sentence back:  cô dạy cực kì hay , rất đỉnh , đưa bài lên trang mạng \n"
     ]
    }
   ],
   "source": [
    "# dịch những từ tiếng anh thành tiếng việt \n",
    "def translate2word(sentence, dictionary = translate_dict):\n",
    "    sentence = \" \" + sentence.strip() + \" \"\n",
    "    for key, value_list in dictionary.items():\n",
    "        for value in value_list:\n",
    "            sentence = sentence.replace(value, key)\n",
    "    return sentence\n",
    "\n",
    "sentence = 'cô dạy max hay , rất pro , đưa bài lên web'\n",
    "print(\"Converted sentence back:\", translate2word(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Remove extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(input_string):\n",
    "    words = input_string.split()\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example usage\n",
    "input_string = \"This    is      a   string     with     extra     spaces.\"\n",
    "print(remove_extra_whitespace(input_string))  # Output: \"This is a string with extra spaces.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove number (digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence'][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(input_string):\n",
    "    # Use the isalpha() method to filter out numeric characters\n",
    "    cleaned_string = ''.join(char for char in input_string if not char.isdigit())\n",
    "    return cleaned_string\n",
    "\n",
    "train_data = train_data.map(lambda example: {'sentence': remove_numbers(example['sentence'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thêm token bình thường\n",
    "import underthesea\n",
    "def word_segment(text):\n",
    "    return underthesea.word_tokenize(text, format=\"text\")\n",
    "\n",
    "word_segment('gíao viên này dạy ko hay như gv khác')\n",
    "#word_segment(\"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(lambda example: {'sentence': word_segment(example['sentence'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Remove stopwords and particular words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text, stopwords_file='stopword.txt'):\n",
    "    # Read the custom stop words from the file\n",
    "    with open(stopwords_file, 'r', encoding='utf-8') as file:\n",
    "        stopwords = set(line.strip() for line in file)\n",
    "\n",
    "    cleaned_words = [word for word in input_text.split() if word.lower() not in stopwords]\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Test the function\n",
    "input_text = \"thầy rất tận tình và đi dạy rất đúng giờ\"\n",
    "cleaned_text = remove_stopwords(input_text, 'stopword.txt')\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(lambda example: {'sentence': remove_stopwords(example['sentence'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Key_clause extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "\n",
    "def key_clause_extraction_vietnamese(text):\n",
    "    sentences = underthesea.sent_tokenize(text)\n",
    "    \n",
    "    key_clauses = []\n",
    "    for sentence in sentences:\n",
    "        # Loại bỏ các câu không chứa thông tin (ví dụ: các câu chỉ chứa một số từ liên kết hoặc từ giới thiệu)\n",
    "        if len(sentence.strip()) > 5:\n",
    "            # Lưu lại câu chính đầu tiên (có thể mở rộng hơn để lựa chọn nhiều câu chính hơn)\n",
    "            key_clauses.append(sentence)\n",
    "            \n",
    "    return key_clauses\n",
    "\n",
    "# Ví dụ văn bản đầu vào (tiếng Việt)\n",
    "text = \"Trí tuệ nhân tạo (AI) là mô phỏng quá trình trí tuệ của con người bởi máy móc, đặc biệt là các hệ thống máy tính. Các quá trình này bao gồm học hỏi (tiếp thu thông tin và quy tắc để sử dụng thông tin đó), lập luận (sử dụng quy tắc để đạt đến kết luận gần đúng hoặc xác định) và tự sửa lỗi. AI có thể được phân loại thành AI yếu và AI mạnh. AI yếu, còn được gọi là AI hẹp, là một hệ thống AI được thiết kế và huấn luyện cho một nhiệm vụ cụ thể. Trợ lý cá nhân ảo, chẳng hạn như Siri của Apple, là một dạng AI yếu. AI mạnh, còn được gọi là trí tuệ nhân tạo tổng quát, là một hệ thống AI có khả năng tổng quát hóa như trí tuệ nhân tạo của con người. Khi được đưa ra một nhiệm vụ không quen thuộc, một hệ thống AI mạnh có thể tìm ra giải pháp mà không cần sự can thiệp của con người.\"\n",
    "#text =\"giảng viên phải nên cho ví dụ cụ thể những phần quan trọng nên nói kỹ có thể lấy ví dụ minh họa tránh việc lập đi lập lại nhiều lần sẽ tạo cảm giác chán cho sinh viên .\"\n",
    "\n",
    "# Trích xuất key clause\n",
    "key_clauses = key_clause_extraction_vietnamese(text)\n",
    "\n",
    "# In kết quả\n",
    "for i, clause in enumerate(key_clauses, start=1):\n",
    "    print(f\"Key Clause {i}: {clause}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _2_1_pre_processing import data_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(lambda example: {'sentence': data_preprocessing(example['sentence'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sentence'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
